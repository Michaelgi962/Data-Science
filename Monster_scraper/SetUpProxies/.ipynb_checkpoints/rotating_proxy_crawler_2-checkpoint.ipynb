{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref: https://codelike.pro/create-a-crawler-with-rotating-ip-proxy-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text data from the first job posting\n",
    "import pandas as pd\n",
    "import requests # For website connections\n",
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no https proxies to import\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Https_txt_file = 'proxyLists/https_proxys_part_2.txt'\n",
    "    #Download New HTTPS IPs .txt file from https://www.proxy-list.download/HTTPS\n",
    "    https_proxys = pd.read_csv(Https_txt_file, sep=\" \", header=None)\n",
    "    #Convert to array\n",
    "    https_proxys = https_proxys[0].values\n",
    "    #Convert to list\n",
    "    https_proxys = list(https_proxys)\n",
    "except:\n",
    "    print('There are no https proxies to import')\n",
    "\n",
    "try:\n",
    "    Http_txt_file = 'proxyLists/http_proxys_part_2.txt'\n",
    "    #Download New HTTPS IPs .txt file from https://www.proxy-list.download/HTTP\n",
    "    http_proxys = pd.read_csv(Http_txt_file, sep=\" \", header=None)\n",
    "    #Convert to array\n",
    "    http_proxys = http_proxys[0].values\n",
    "    #Convert to list\n",
    "    http_proxys = list(http_proxys)\n",
    "except:\n",
    "    print('There are no http proxies to import')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary for the proxies to test\n",
    "testProxies = {'https':https_proxys,'http':http_proxys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a random user-agent\n",
    "header = {}\n",
    "def randHeader():\n",
    "    ua = UserAgent()\n",
    "    header['User-Agent'] = ua.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a random index for the https proxy\n",
    "def randomIndex(conn_type):\n",
    "    num = random.randint(0, len(testProxies[conn_type]) - 1)\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxyDict = {}\n",
    "def randProx(conn_type):\n",
    "    # Choose a single random proxy to use\n",
    "    proxy_index = randomIndex(conn_type)\n",
    "    proxy = testProxies[conn_type][proxy_index]\n",
    "    proxyDict[conn_type] = conn_type +'://'+ proxy\n",
    "    return proxy_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dictionary for good proxies \n",
    "good_http_proxy = []\n",
    "good_https_proxy = []\n",
    "conn_time_http = []\n",
    "conn_time_https = []\n",
    "goodProxy = {'https':{'proxy':good_https_proxy,'time':conn_time_https},\n",
    "             'http':{'proxy':good_http_proxy,'time':conn_time_http}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterProxys(url):\n",
    "    timer = 0\n",
    "    count = 0\n",
    "    if url[0:5] == 'http:':\n",
    "        conn_type = 'http'\n",
    "        \n",
    "    elif url[0:5] == 'https':\n",
    "        conn_type = 'https'\n",
    "\n",
    "    total_proxies = len(testProxies[conn_type])\n",
    "    for n in range(len(testProxies[conn_type])):\n",
    "        time.sleep(1)\n",
    "        print(str(total_proxies-n)+' proxies left to filter')\n",
    "            \n",
    "        try:\n",
    "            # Choose a random header\n",
    "            randHeader()\n",
    "            \n",
    "        except:\n",
    "            # Sometimes the header reassignment function returns an error\n",
    "            pass\n",
    "            \n",
    "        # Choose a random proxy\n",
    "        proxy_index = randProx(conn_type)\n",
    "\n",
    "        try:\n",
    "            # Try to connect to the url\n",
    "            t1 = time.time()\n",
    "            soup = requests.get(url,headers=header,proxies=proxyDict,timeout=5)\n",
    "            t2 = time.time()\n",
    "            t = t2-t1\n",
    "                \n",
    "            if  str(soup) == '<Response [200]>': # the connection is successful when <Response [200]> is returned\n",
    "                print(str(testProxies[conn_type][proxy_index])+' is good')\n",
    "                del testProxies[conn_type][proxy_index]\n",
    "                timer += t\n",
    "                count += 1\n",
    "                print('average conn time: '+str(timer/count)+'\\n')\n",
    "                #Add to the good proxy and its connection time\n",
    "                goodProxy[conn_type]['proxy'].append(proxyDict[conn_type])\n",
    "                goodProxy[conn_type]['time'].append(t)\n",
    "                    \n",
    "            elif str(soup) != '<Response [200]>': #The connection was bad\n",
    "                print(str(testProxies[conn_type][proxy_index])+' is bad'+'\\n')\n",
    "                del testProxies[conn_type][proxy_index]\n",
    "                    \n",
    "                    \n",
    "        except: # The connection took too long \n",
    "            print(str(testProxies[conn_type][proxy_index])+' is slow'+'\\n')\n",
    "            del testProxies[conn_type][proxy_index]\n",
    "    try:\n",
    "        print('average proxy time:'+str(timer/count))\n",
    "    except:\n",
    "        print('No proxies to text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the Https proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 proxies left to filter\n",
      "218.102.119.7:8380 is slow\n",
      "\n",
      "No proxies to text\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.proxy-list.download/HTTPS'\n",
    "filterProxys(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the Http proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.proxy-list.download/HTTP'\n",
    "filterProxys(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overwrite the source text file with partially filtered or empty ip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the existing source of https proxy data with the filtered data\n",
    "with open('proxyLists/https_proxys_part_2.txt', \"w\") as output:\n",
    "    for line in testProxies['https']:\n",
    "        output.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the existing source of http proxy data with the filtered data\n",
    "with open('proxyLists/http_proxys_part_2.txt', \"w\") as output:\n",
    "    for line in testProxies['http']:\n",
    "        output.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all proxys that had connections times less than one second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(goodProxy['https'])\n",
    "fast_proxies_https = list(df['proxy'][df['time']<1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(goodProxy['http'])\n",
    "fast_proxies_http = list(df['proxy'][df['time']<1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the single filtered files of proxys that had connections times less than one second and combine with the new filtered proxys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "try:\n",
    "    existing_https_proxies = list(pd.read_csv('../filtered_https_proxys.txt', header = None)[0])\n",
    "except: #If the file is empty\n",
    "    existing_https_proxies = []\n",
    "try:\n",
    "    existing_http_proxies = list(pd.read_csv('../filtered_http_proxys.txt', header = None)[0])\n",
    "except: #If the file is empty\n",
    "    existing_http_proxies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the new proxys to the existing ones if they are not already there\n",
    "[existing_https_proxies.append(x) \n",
    " for x in fast_proxies_https \n",
    " if x not in existing_https_proxies]\n",
    "\n",
    "[existing_http_proxies.append(x) \n",
    " for x in fast_proxies_http \n",
    " if x not in existing_http_proxies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewrite the final proxys text file with the new and old filtered proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append filtered http proxys to existing .txt file\n",
    "with open('../filtered_http_proxys.txt', \"w\") as output:\n",
    "    for line in existing_http_proxies:\n",
    "        output.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append filtered https proxies to existing text file\n",
    "with open('../filtered_https_proxys.txt', \"w\") as output:\n",
    "    for line in existing_https_proxies:\n",
    "        output.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
