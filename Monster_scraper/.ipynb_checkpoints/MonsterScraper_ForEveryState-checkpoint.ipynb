{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Data From Monster For All States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # For Data Storage\n",
    "import requests # For website connection\n",
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import time # For sleep \n",
    "import re # Regular expressions for removing non-ascii terms\n",
    "from itertools import groupby # For removing duplicates from lists\n",
    "import math # Need ceiling expression\n",
    "from nltk.corpus import stopwords # For filtering out words like 'is', 'the', 'of'\n",
    "stop_words = set(stopwords.words(\"english\")) #initialize stopwords    \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Empty Lists (For Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Empty State Set\n",
    "state = []\n",
    "job_title = []\n",
    "\n",
    "#Initialize Empty Skills List\n",
    "r = []\n",
    "python = []\n",
    "java = []\n",
    "cpp = []\n",
    "csharp = [] \n",
    "c = [] \n",
    "ruby = []\n",
    "perl = []\n",
    "sas = [] \n",
    "spss = []\n",
    "matlab = []\n",
    "javascript = []\n",
    "scala = []\n",
    "excel = []\n",
    "vba = []\n",
    "tableau = []\n",
    "d3 = []\n",
    "hadoop = []\n",
    "mapreduce = []\n",
    "spark = []\n",
    "pig = []\n",
    "hive = []\n",
    "shark = []\n",
    "oozie = []\n",
    "zookeeper = []\n",
    "flume = []\n",
    "mahout = []\n",
    "sql = [] \n",
    "mysql = []\n",
    "tsql = []\n",
    "oracle = []\n",
    "postgresql = []\n",
    "nosql =[]\n",
    "sybase = []\n",
    "rethinkdb = []\n",
    "memcacheddb = []\n",
    "couchdb = []\n",
    "hbase = []\n",
    "cassandra = []\n",
    "mongodb = []\n",
    "db2 = []\n",
    "git = []\n",
    "tensorflow = []\n",
    "keras = []\n",
    "\n",
    "skills = ['r',\n",
    "'python',\n",
    "'java',\n",
    "'c++',\n",
    "'c#',\n",
    "'c',\n",
    "'ruby',\n",
    "'perl',\n",
    "'sas',\n",
    "'spss',\n",
    "'matlab',\n",
    "'javascript',\n",
    "'scala', \n",
    "'excel',\n",
    "'vba',\n",
    "'tableau',\n",
    "'d3',\n",
    "'hadoop',\n",
    "'mapreduce',\n",
    "'spark',\n",
    "'pig',\n",
    "'hive',\n",
    "'shark',\n",
    "'oozie',\n",
    "'zookeeper',\n",
    "'flume',\n",
    "'mahout',\n",
    "'sql',\n",
    "'tsql',\n",
    "'mysql',\n",
    "'oracle',\n",
    "'postgresql',\n",
    "'nosql',\n",
    "'sybase',\n",
    "'rethinkdb',\n",
    "'memcacheddb',\n",
    "'couchdb',\n",
    "'hbase',\n",
    "'cassandra',\n",
    "'mongodb',\n",
    "'db2',\n",
    "'git',\n",
    "'tensorflow',\n",
    "'keras'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    'job_title':job_title,\n",
    "    'state':state,\n",
    "    'r':r,\n",
    "    'python':python,\n",
    "    'java':java,\n",
    "    'c++':cpp,\n",
    "    'c#':csharp,\n",
    "    'c':c,\n",
    "    'ruby':ruby,\n",
    "    'perl':perl,\n",
    "    'sas':sas,\n",
    "    'spss':spss,\n",
    "    'matlab':matlab,\n",
    "    'javascript':javascript,\n",
    "    'scala':scala,\n",
    "    'excel':excel,\n",
    "    'vba':vba,\n",
    "    'tableau':tableau,\n",
    "    'd3':d3,\n",
    "    'hadoop':hadoop,\n",
    "    'mapreduce':mapreduce,\n",
    "    'spark':spark,\n",
    "    'pig':pig,\n",
    "    'hive':hive,\n",
    "    'shark':shark,\n",
    "    'oozie':oozie,\n",
    "    'zookeeper':zookeeper,\n",
    "    'flume':flume,\n",
    "    'mahout':mahout,\n",
    "    'sql':sql, \n",
    "    'tsql':tsql,\n",
    "    'mysql':mysql,\n",
    "    'oracle':oracle,\n",
    "    'postgresql':postgresql,\n",
    "    'nosql':nosql,\n",
    "    'sybase':sybase,\n",
    "    'rethinkdb':rethinkdb,\n",
    "    'memcacheddb':memcacheddb,\n",
    "    'couchdb':couchdb,\n",
    "    'hbase':hbase,\n",
    "    'cassandra':cassandra,\n",
    "    'mongodb':mongodb,\n",
    "    'db2':db2,\n",
    "    'git':git,\n",
    "    'tensorflow':tensorflow,\n",
    "    'keras':keras}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Func: Add A Row of Data To Dictionary (For a Job Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addValsToDict(url_state,url_job_title):\n",
    "    #Append job_title\n",
    "    dictionary['job_title'].append(url_job_title)\n",
    "    \n",
    "    #Append state\n",
    "    dictionary['state'].append(url_state)\n",
    "\n",
    "    #Check if the each skill is in the wrangled text\n",
    "    for skill in skills:\n",
    "        if (skill in text_final) == True:\n",
    "            dictionary[skill].append(1)\n",
    "        else:\n",
    "            dictionary[skill].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define States List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import proxys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Get top 10 proxies from http://free-proxy.cz/en/proxylist/country/US/https/ping/level1\n",
    "https_proxys = ['34.192.220.22:808',\n",
    "              '206.189.112.106:3128',\n",
    "              '209.97.191.169:3128',\n",
    "              '157.230.34.190:1111',\n",
    "              '157.230.45.121:1111',\n",
    "              '23.108.64.65:8118',\n",
    "              '157.230.33.37:1111',\n",
    "              '52.128.60.130:50692',\n",
    "              '12.218.209.130:53281',\n",
    "              '38.134.10.106:53281'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proxies from filter\n",
    "https_proxys = list(pd.read_csv('filtered_https_proxys.txt',header=None)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents = pd.read_csv('headers.txt',delimiter = '\\t',header=None).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents = user_agents[0:-1] # Get rid of last element (it is nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents = list(user_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create select a random proxy and header funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randIndex(lengthList):\n",
    "    index = random.randint(0,lengthList-1)\n",
    "    return index\n",
    "\n",
    "def randProxy():\n",
    "    index = randIndex(len(https_proxys))\n",
    "    return https_proxys[index]\n",
    "    \n",
    "def randHeader():\n",
    "    index = randIndex(len(user_agents))\n",
    "    return user_agents[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Headers Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'User-Agent':randHeader()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Proxy Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = {'https':randProxy()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Each State:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Get the urls that lists up to 250 job openings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the general search and use dashes ('-') in place of spaces\n",
    "search_term = 'econometrics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states left to scrape: 49\n",
      "Number of states left to scrape: 48\n",
      "could not convert string to float: \n",
      "Number of states left to scrape: 47\n",
      "Number of states left to scrape: 46\n",
      "Number of states left to scrape: 45\n",
      "Number of states left to scrape: 44\n",
      "('Connection aborted.', BadStatusLine(\"''\",))\n",
      "Number of states left to scrape: 44\n",
      "Number of states left to scrape: 43\n",
      "Number of states left to scrape: 42\n",
      "Number of states left to scrape: 41\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=GA&stpage=1 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d38e450>, 'Connection to 104.248.168.59 timed out. (connect timeout=5)'))\n",
      "Number of states left to scrape: 41\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=HI&stpage=1 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of states left to scrape: 41\n",
      "('Connection aborted.', BadStatusLine(\"''\",))\n",
      "Number of states left to scrape: 41\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=IL&stpage=1 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of states left to scrape: 41\n",
      "Number of states left to scrape: 40\n",
      "Number of states left to scrape: 39\n",
      "could not convert string to float: \n",
      "Number of states left to scrape: 38\n",
      "Number of states left to scrape: 37\n",
      "could not convert string to float: \n",
      "Number of states left to scrape: 36\n",
      "Number of states left to scrape: 35\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=MD&stpage=1 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d8ae850>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of states left to scrape: 35\n",
      "Number of states left to scrape: 34\n",
      "Number of states left to scrape: 33\n",
      "Number of states left to scrape: 32\n",
      "could not convert string to float: \n",
      "Number of states left to scrape: 31\n",
      "Number of states left to scrape: 30\n",
      "Number of states left to scrape: 29\n",
      "('Connection aborted.', BadStatusLine(\"''\",))\n",
      "Number of states left to scrape: 29\n",
      "Number of states left to scrape: 28\n",
      "could not convert string to float: \n",
      "Number of states left to scrape: 27\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=NJ&stpage=1 (Caused by ProxyError('Cannot connect to proxy.', error('Tunnel connection failed: 400 Bad Request',)))\n",
      "Number of states left to scrape: 27\n",
      "Number of states left to scrape: 26\n",
      "Number of states left to scrape: 25\n",
      "Number of states left to scrape: 24\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=ND&stpage=1 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d376290>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of states left to scrape: 24\n",
      "Number of states left to scrape: 23\n",
      "could not convert string to float: \n",
      "Number of states left to scrape: 22\n",
      "Number of states left to scrape: 21\n",
      "Number of states left to scrape: 20\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=RI&stpage=1 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d78dd10>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of states left to scrape: 20\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=SC&stpage=1 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1dfcc490>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of states left to scrape: 20\n",
      "could not convert string to float: \n",
      "Number of states left to scrape: 19\n",
      "Number of states left to scrape: 18\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=TX&stpage=1 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d15e990>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of states left to scrape: 18\n",
      "Number of states left to scrape: 17\n",
      "Number of states left to scrape: 16\n",
      "Number of states left to scrape: 15\n",
      "Number of states left to scrape: 14\n",
      "could not convert string to float: \n",
      "Number of states left to scrape: 13\n",
      "Number of states left to scrape: 12\n",
      "could not convert string to float: \n",
      "Number of states left to scrape: 11\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=CO&stpage=1 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1da36350>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of states left to scrape: 11\n",
      "Number of states left to scrape: 10\n",
      "Number of states left to scrape: 9\n",
      "Number of states left to scrape: 8\n",
      "Number of states left to scrape: 7\n",
      "Number of states left to scrape: 6\n",
      "Number of states left to scrape: 5\n",
      "Number of states left to scrape: 4\n",
      "could not convert string to float: \n",
      "Number of states left to scrape: 3\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=RI&stpage=1 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1ddcfa10>: Failed to establish a new connection: [Errno 51] Network is unreachable',)))\n",
      "Number of states left to scrape: 3\n",
      "Number of states left to scrape: 2\n",
      "Number of states left to scrape: 1\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=CO&stpage=1 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d2bfc90>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of states left to scrape: 1\n",
      "('Connection aborted.', BadStatusLine(\"''\",))\n",
      "Number of states left to scrape: 1\n",
      "Number of states left to scrape: 0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "# Initialize Empty URL Dictionary\n",
    "final_state_url_dict = {} \n",
    "\n",
    "while len(final_state_url_dict.keys()) < 50:\n",
    "    for url_state in states:\n",
    "        try:\n",
    "            if url_state not in final_state_url_dict.keys():\n",
    "                time.sleep(1) # sleep 1 second\n",
    "\n",
    "                # Assign random proxy\n",
    "                proxy['https'] = randProxy()\n",
    "\n",
    "                # Assign random header\n",
    "                header['User-Agent'] = randHeader()\n",
    "\n",
    "                count += 1\n",
    "                print(\"Number of states left to scrape: \"+str(50-count))\n",
    "\n",
    "                url = 'https://www.monster.com/jobs/search/?q='+search_term+'&where='+url_state+'&stpage=1'        \n",
    "\n",
    "                response = requests.get(url, proxies=proxy, headers=header, timeout=5) #Try to get the html code\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\") #Store html in a soup object \n",
    "\n",
    "                time.sleep(1) #pause code for 1 sec\n",
    "                \n",
    "                # Sometimes there are no job openings and the float conversion throws an error\n",
    "                # When this float conversion error occurs, we set the total total job openings equal to zero\n",
    "                try: \n",
    "                    #Get the total number of listed jobs \n",
    "                    totalJobOpenings = float(re.sub('[^0-9]','',soup.html.body.find_all('h2')[0].text.strip()))\n",
    "                \n",
    "                except Exception,e: \n",
    "                    print str(e)\n",
    "                    totalJobOpenings = 0\n",
    "                    print url_state+' has no jobs.'\n",
    "                    \n",
    "                #Get the total number of pages (up to 10) and a max of 25 per page\n",
    "                totalPages = int(min(math.ceil(totalJobOpenings/25),10))\n",
    "\n",
    "                #Store the Final URL in a dictionary each state\n",
    "                final_state_url_dict[url_state] = 'https://www.monster.com/jobs/search/?q='+search_term+'&where='+url_state+'&stpage=1&page='+str(totalPages)\n",
    "\n",
    "\n",
    "        except Exception,e: \n",
    "            count -= 1\n",
    "            print str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Get the list of job post URLs -> For each URL: check if each skill exists and store data in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states left to scrape: 50\n",
      "Number of jobs left to scrape: 108\n",
      "Number of jobs left to scrape: 107\n",
      "Number of jobs left to scrape: 106\n",
      "Number of jobs left to scrape: 105\n",
      "Number of jobs left to scrape: 104\n",
      "Number of jobs left to scrape: 103\n",
      "Number of jobs left to scrape: 102\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /sr-data-scientist-sql-r-sas-financial-industry-seattle-wa-direct-hire-seattle-wa-us-kelly-services/7433507e-4997-47e0-bd93-04fcac3d7a13 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of jobs left to scrape: 102\n",
      "Number of jobs left to scrape: 101\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /data-applied-scientist-redmond-wa-us-microsoft-corporation/32f2fb32-b75e-41d2-b560-8e958750ae95 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1083d3250>, 'Connection to 217.69.10.117 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 101\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /operations-research-scientist-seattle-wa-us-amazon-web-services/3d5b649c-e18c-4252-83d4-1a432e9e65db (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1083d3390>, 'Connection to 191.102.106.1 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 101\n",
      "Number of jobs left to scrape: 100\n",
      "Number of jobs left to scrape: 99\n",
      "Number of jobs left to scrape: 98\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /epic-clarity-health-insights-clinical-analyst-%E2%80%93-psjh-renton-wa-us-providence-health-services/52332682-d7be-4c4c-9ae0-399077f1768f (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x106ebaa90>, 'Connection to 163.172.86.172 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 98\n",
      "Number of jobs left to scrape: 97\n",
      "Number of jobs left to scrape: 96\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-data-scientist-seattle-wa-us-washington-seattle/206848231 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1cfb9390>, 'Connection to 51.158.69.48 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 96\n",
      "Number of jobs left to scrape: 95\n",
      "Number of jobs left to scrape: 94\n",
      "Number of jobs left to scrape: 93\n",
      "Number of jobs left to scrape: 92\n",
      "Number of jobs left to scrape: 91\n",
      "Number of jobs left to scrape: 90\n",
      "Number of jobs left to scrape: 89\n",
      "('Connection aborted.', BadStatusLine(\"''\",))\n",
      "Number of jobs left to scrape: 89\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /sr-manager-software-development-data-engineering-seattle-wa-us-amazon-web-services/6f7e332c-9948-492d-a8ec-b43b2920e952 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of jobs left to scrape: 89\n",
      "Number of jobs left to scrape: 88\n",
      "Number of jobs left to scrape: 87\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /lead-data-scientist-seattle-wa-us-compucom/ddf17161-0853-484f-aa2d-105491160f82 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of jobs left to scrape: 87\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-economist-seattle-wa-us-amazon-web-services/7825a85e-625d-41dd-9657-ee400c8bb79b (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d0cf250>, 'Connection to 54.84.154.208 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 87\n",
      "Number of jobs left to scrape: 86\n",
      "Number of jobs left to scrape: 85\n",
      "Number of jobs left to scrape: 84\n",
      "Number of jobs left to scrape: 83\n",
      "Number of jobs left to scrape: 82\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-technical-program-manager-seattle-wa-us-amazon-web-services/e192a100-93f6-425f-8940-b53b21ee4780 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1e2d6310>, 'Connection to 51.83.31.150 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 82\n",
      "Number of jobs left to scrape: 81\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-data-scientist-risk-seattle-wa-us-zillow/8d055284-a4c8-48b1-9c5b-411fe18a7aec (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1ddcfa90>, 'Connection to 104.248.89.208 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 81\n",
      "Number of jobs left to scrape: 80\n",
      "Number of jobs left to scrape: 79\n",
      "Number of jobs left to scrape: 78\n",
      "Number of jobs left to scrape: 77\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /amazon-music-finance-bie-seattle-wa-us-amazon-web-services/8c87b371-bcc3-49c7-9642-4e94bd9e9169 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1dc11310>, 'Connection to 217.69.10.117 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 77\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /economist-data-science-seattle-wa-us-convoy/c346723e-4b99-4e81-8690-e2bb383523e7 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of jobs left to scrape: 77\n",
      "Number of jobs left to scrape: 76\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /principal-data-applied-scientist-redmond-wa-us-microsoft-corporation/521ed89f-b75c-47f2-bb6f-670a346bc6e0 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1ceac910>, 'Connection to 217.69.10.117 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 76\n",
      "Number of jobs left to scrape: 75\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-marketing-analyst-kent-wa-us-rei/7d782c14-7286-4d17-9624-c4a3fdb921ec (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1e315190>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 75\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /sr-manager-software-development-seattle-wa-us-amazon-web-services/6ffe97f4-3056-49e9-998c-9cf4efd592a0 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1e315290>, 'Connection to 104.248.89.208 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 75\n",
      "Number of jobs left to scrape: 74\n",
      "Number of jobs left to scrape: 73\n",
      "('Connection aborted.', BadStatusLine(\"''\",))\n",
      "Number of jobs left to scrape: 73\n",
      "Number of jobs left to scrape: 72\n",
      "Number of jobs left to scrape: 71\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /operational-insights-manager-bellevue-wa-us-t-mobile/2df9fb12-7762-4ae6-bd33-dc2b6fd83a00 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1da0be10>, 'Connection to 217.69.10.117 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 71\n",
      "Number of jobs left to scrape: 70\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /sr-manager-product-mgr-tech-seattle-wa-us-amazon-web-services/785f7f09-bb69-4ebd-b910-91468e3f3116 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1dfc1990>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 70\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /software-dev-engineer-seattle-wa-us-amazon-web-services/6e8e7fb6-a2c3-44d8-b15d-6db9462d2790 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of jobs left to scrape: 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs left to scrape: 69\n",
      "Number of jobs left to scrape: 68\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-data-scientist-seattle-wa-us-kelly-services/df17d91c-5256-4eb5-9cf7-89c1fcf2c4a9 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1dc113d0>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 68\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-data-applied-scientist-redmond-wa-us-microsoft-corporation/9ad87df7-386e-4704-885c-82d2fc8c23fa (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1cedbc50>, 'Connection to 54.84.154.208 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 68\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-product-manager-seattle-wa-us-amazon-web-services/9426ac67-2c61-48e0-b71d-ea5a673a83df (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1cedb9d0>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 68\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-data-analyst-data-science-seattle-wa-us-convoy/ccbc697f-0d3e-4bf5-9526-cb1a20f6ec7b (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1cedb450>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 68\n",
      "Number of jobs left to scrape: 67\n",
      "Number of jobs left to scrape: 66\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /business-development-manager-seattle-wa-us-arvato-systems-north-america-inc/206988870 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d228310>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 66\n",
      "Number of jobs left to scrape: 65\n",
      "Number of jobs left to scrape: 64\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /director-customer-success-analytics-seattle-wa-us-docusign-inc/0bc4a3f5-e359-4e0f-ac23-adfb5886e8bf (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1da36a90>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 64\n",
      "Number of jobs left to scrape: 63\n",
      "Number of jobs left to scrape: 62\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /data-scientist-financial-services-seattle-wa-us-kelly-services/abecae57-dda2-41a1-967f-ff6061b9c529 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1e255ed0>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 62\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-data-scientist-seattle-wa-us-cambia-health-solutions/33aae906-f730-45f0-b53b-1cea96ba6d20 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1e255e50>, 'Connection to 104.248.168.59 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 62\n",
      "list index out of range\n",
      "Number of jobs left to scrape: 62\n",
      "Number of jobs left to scrape: 61\n",
      "('Connection aborted.', BadStatusLine(\"''\",))\n",
      "Number of jobs left to scrape: 61\n",
      "Number of jobs left to scrape: 60\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /sr-data-scientist-sql-r-sas-financial-industry-seattle-wa-direct-hire-seattle-wa-us-kelly-services/1d849e37-c37f-4036-b923-171112ac4074 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d353f50>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 60\n",
      "Number of jobs left to scrape: 59\n",
      "Number of jobs left to scrape: 58\n",
      "Number of jobs left to scrape: 57\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /intern-economics-seattle-wa-us-amazon-web-services/228faad5-5f4e-4f7e-b0e6-40a26e64084b (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1cf6e050>, 'Connection to 51.83.31.150 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 57\n",
      "Number of jobs left to scrape: 56\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-data-scientist-seattle-wa-us-cambia-health-solutions-inc/304bf023-391e-4773-85b5-cdc5f91d9dcb (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of jobs left to scrape: 56\n",
      "Number of jobs left to scrape: 55\n",
      "Number of jobs left to scrape: 54\n",
      "Number of jobs left to scrape: 53\n",
      "Number of jobs left to scrape: 52\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /sr-data-scientist-sql-r-sas-financial-industry-seattle-wa-direct-hire-seattle-wa-us-kelly-services/4b65f747-3ee4-4b32-890d-056d5d0db37a (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1e00e390>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 52\n",
      "Number of jobs left to scrape: 51\n",
      "Number of jobs left to scrape: 50\n",
      "Number of jobs left to scrape: 49\n",
      "Number of jobs left to scrape: 48\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /applied-scientist-seattle-wa-us-amazon-web-services/52f0df14-63da-48cd-a911-7b53a9a7111f (Caused by ProxyError('Cannot connect to proxy.', error('Tunnel connection failed: 403 Forbidden',)))\n",
      "Number of jobs left to scrape: 48\n",
      "Number of jobs left to scrape: 47\n",
      "Number of jobs left to scrape: 46\n",
      "Number of jobs left to scrape: 45\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /manager-data-analytics-seattle-wa-us-convoy/70b9ba71-13e1-43d8-a165-dfa85c339de4 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1083d3a50>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 45\n",
      "Number of jobs left to scrape: 44\n",
      "Number of jobs left to scrape: 43\n",
      "Number of jobs left to scrape: 42\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /economist-seattle-wa-us-amazon-web-services/d25e1723-d028-4143-a326-501896e0246d (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1daa9c50>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 42\n",
      "Number of jobs left to scrape: 41\n",
      "Number of jobs left to scrape: 40\n",
      "Number of jobs left to scrape: 39\n",
      "('Connection aborted.', BadStatusLine(\"''\",))\n",
      "Number of jobs left to scrape: 39\n",
      "Number of jobs left to scrape: 38\n",
      "Number of jobs left to scrape: 37\n",
      "Number of jobs left to scrape: 36\n",
      "Number of jobs left to scrape: 35\n",
      "Number of jobs left to scrape: 34\n",
      "Number of jobs left to scrape: 33\n",
      "Number of jobs left to scrape: 32\n",
      "Number of jobs left to scrape: 31\n",
      "Number of jobs left to scrape: 30\n",
      "Number of jobs left to scrape: 29\n",
      "Number of jobs left to scrape: 28\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-data-scientist-risk-seattle-wa-us-zillow/8d055284-a4c8-48b1-9c5b-411fe18a7aec (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d325650>, 'Connection to 217.69.10.117 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /amazon-music-finance-bie-seattle-wa-us-amazon-web-services/8c87b371-bcc3-49c7-9642-4e94bd9e9169 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d325350>, 'Connection to 163.172.86.172 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 28\n",
      "Number of jobs left to scrape: 27\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /principal-data-applied-scientist-redmond-wa-us-microsoft-corporation/521ed89f-b75c-47f2-bb6f-670a346bc6e0 (Caused by ProxyError('Cannot connect to proxy.', error('Tunnel connection failed: 403 Forbidden',)))\n",
      "Number of jobs left to scrape: 27\n",
      "Number of jobs left to scrape: 26\n",
      "Number of jobs left to scrape: 25\n",
      "Number of jobs left to scrape: 24\n",
      "Number of jobs left to scrape: 23\n",
      "Number of jobs left to scrape: 22\n",
      "Number of jobs left to scrape: 21\n",
      "Number of jobs left to scrape: 20\n",
      "Number of jobs left to scrape: 19\n",
      "Number of jobs left to scrape: 18\n",
      "Number of jobs left to scrape: 17\n",
      "Number of jobs left to scrape: 16\n",
      "Number of jobs left to scrape: 15\n",
      "Number of jobs left to scrape: 14\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-data-scientist-seattle-wa-us-cambia-health-solutions/33aae906-f730-45f0-b53b-1cea96ba6d20 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d27ed10>, 'Connection to 163.172.86.172 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 14\n",
      "Number of jobs left to scrape: 13\n",
      "Number of jobs left to scrape: 12\n",
      "Number of jobs left to scrape: 11\n",
      "Number of jobs left to scrape: 10\n",
      "Number of jobs left to scrape: 9\n",
      "Number of jobs left to scrape: 8\n",
      "Number of jobs left to scrape: 7\n",
      "Number of jobs left to scrape: 6\n",
      "Number of jobs left to scrape: 5\n",
      "Number of jobs left to scrape: 4\n",
      "('Connection aborted.', BadStatusLine(\"''\",))\n",
      "Number of jobs left to scrape: 4\n",
      "Number of jobs left to scrape: 3\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /principal-data-applied-scientist-redmond-wa-us-microsoft-corporation/521ed89f-b75c-47f2-bb6f-670a346bc6e0 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1dfc19d0>, 'Connection to 217.69.10.117 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 3\n",
      "Number of jobs left to scrape: 2\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-data-scientist-risk-seattle-wa-us-zillow/8d055284-a4c8-48b1-9c5b-411fe18a7aec (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1e288f90>, 'Connection to 163.172.86.172 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 2\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /principal-data-applied-scientist-redmond-wa-us-microsoft-corporation/521ed89f-b75c-47f2-bb6f-670a346bc6e0 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1e194790>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 2\n",
      "Number of jobs left to scrape: 1\n",
      "Number of states left to scrape: 49\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=DE&stpage=1&page=1 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of states left to scrape: 49\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=WI&stpage=1&page=1 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of states left to scrape: 49\n",
      "WVhas no jobs\n",
      "Number of states left to scrape: 48\n",
      "Number of jobs left to scrape: 5\n",
      "Number of jobs left to scrape: 4\n",
      "Number of jobs left to scrape: 3\n",
      "Number of jobs left to scrape: 2\n",
      "Number of jobs left to scrape: 1\n",
      "Number of states left to scrape: 47\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=WI&stpage=1&page=1 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1da1dc90>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of states left to scrape: 47\n",
      "Number of jobs left to scrape: 1\n",
      "Number of states left to scrape: 46\n",
      "Number of jobs left to scrape: 27\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /avp-model-validation-senior-analyst-job-number-19008991-tampa-fl-us-citigroup-inc/967c0314-a64f-4b88-97b6-92ff1e2b31ec (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of jobs left to scrape: 27\n",
      "Number of jobs left to scrape: 26\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /data-integration-manager-lake-buena-vista-fl-us-the-walt-disney-company/4082c655-b9b3-4713-ba7b-9147efc19e1e (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of jobs left to scrape: 26\n",
      "Number of jobs left to scrape: 25\n",
      "Number of jobs left to scrape: 24\n",
      "Number of jobs left to scrape: 23\n",
      "Number of jobs left to scrape: 22\n",
      "Number of jobs left to scrape: 21\n",
      "Number of jobs left to scrape: 20\n",
      "Number of jobs left to scrape: 19\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /lead-consultant-predictive-prescriptive-analytics-lake-mary-fl-us-verizon/de45d83b-364f-4577-87f6-40082879f5f4 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d67ce90>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 19\n",
      "Number of jobs left to scrape: 18\n",
      "Number of jobs left to scrape: 17\n",
      "Number of jobs left to scrape: 16\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /data-integration-manager-lake-buena-vista-fl-us-the-walt-disney-company/547ab81f-7fa0-43a7-9fdb-c066bfec9ca6 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1083cbf90>, 'Connection to 51.158.69.48 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 16\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /senior-finacial-analyst-fort-lauderdale-fl-us-accountemps/206801955 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of jobs left to scrape: 16\n",
      "Number of jobs left to scrape: 15\n",
      "Number of jobs left to scrape: 14\n",
      "Number of jobs left to scrape: 13\n",
      "Number of jobs left to scrape: 12\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /career-opportunity-tampa-fl-us-citigroup-inc/41b288f9-b120-4726-bd67-b9ce313fbd70 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d653d10>, 'Connection to 51.254.50.238 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 12\n",
      "Number of jobs left to scrape: 11\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /statistical-modeler-credit-lake-mary-fl-us-verizon/703549a4-036b-40a0-93ee-9f1cc27488bf (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1d65c1d0>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 11\n",
      "Number of jobs left to scrape: 10\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /managing-consultant-lake-buena-vista-fl-us-the-walt-disney-company/9594a0b8-2732-46b0-affd-aaeb2d3117fb (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x106ecf8d0>, 'Connection to 54.84.154.208 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 10\n",
      "Number of jobs left to scrape: 9\n",
      "Number of jobs left to scrape: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs left to scrape: 7\n",
      "('Connection aborted.', BadStatusLine(\"''\",))\n",
      "Number of jobs left to scrape: 7\n",
      "Number of jobs left to scrape: 6\n",
      "Number of jobs left to scrape: 5\n",
      "Number of jobs left to scrape: 4\n",
      "Number of jobs left to scrape: 3\n",
      "Number of jobs left to scrape: 2\n",
      "Number of jobs left to scrape: 1\n",
      "Number of states left to scrape: 45\n",
      "WYhas no jobs\n",
      "Number of states left to scrape: 44\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=WI&stpage=1&page=1 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x106ec2610>, 'Connection to 3.17.175.232 timed out. (connect timeout=3)'))\n",
      "Number of states left to scrape: 44\n",
      "NHhas no jobs\n",
      "Number of states left to scrape: 43\n",
      "Number of jobs left to scrape: 6\n",
      "Number of jobs left to scrape: 5\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /insights-analytics-analyst-neenah-wi-us-kimberly-clark-corporation/20e7b777-fa49-4ca5-8a45-202afb6b73d0 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1da1db10>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 5\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /manager-decision-analytics-menomonee-falls-wi-us-kohls/1a7cd86c-c0e6-4795-803b-0b3c75e1ed8a (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out',)))\n",
      "Number of jobs left to scrape: 5\n",
      "Number of jobs left to scrape: 4\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /forecasting-analytics-manager-racine-wi-us-iconma-llc/1d8a07b6-9935-4bc9-899f-26e5be1fe3c5 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1dcdf590>, 'Connection to 104.248.89.208 timed out. (connect timeout=3)'))\n",
      "Number of jobs left to scrape: 4\n",
      "Number of jobs left to scrape: 3\n",
      "Number of jobs left to scrape: 2\n",
      "Number of jobs left to scrape: 1\n",
      "Number of states left to scrape: 42\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=NJ&stpage=1&page=1 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1e2ee750>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of states left to scrape: 42\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=NM&stpage=1&page=1 (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1e2ee850>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of states left to scrape: 42\n",
      "HTTPSConnectionPool(host='www.monster.com', port=443): Max retries exceeded with url: /jobs/search/?q=econometrics&where=TX&stpage=1&page=3 (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x1a1e2ee2d0>, 'Connection to 54.84.154.208 timed out. (connect timeout=3)'))\n",
      "Number of states left to scrape: 42\n",
      "LAhas no jobs\n",
      "Number of states left to scrape: 41\n",
      "Number of jobs left to scrape: 25\n",
      "Number of jobs left to scrape: 24\n",
      "list index out of range\n",
      "Number of jobs left to scrape: 24\n",
      "Number of jobs left to scrape: 23\n",
      "Number of jobs left to scrape: 22\n",
      "Number of jobs left to scrape: 21\n",
      "Number of jobs left to scrape: 20\n",
      "Number of jobs left to scrape: 19\n",
      "Number of jobs left to scrape: 18\n",
      "HTTPSConnectionPool(host='job-openings.monster.com', port=443): Max retries exceeded with url: /data-scientist-rockleigh-nj-us-crestron-electronics-inc/60901b1b-e3cb-4f3a-be9a-353c338083bc (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1083da4d0>: Failed to establish a new connection: [Errno 61] Connection refused',)))\n",
      "Number of jobs left to scrape: 18\n",
      "Number of jobs left to scrape: 17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7e2f0b119c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of jobs left to scrape: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_titles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Get the html code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Store html in a soup object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                         \u001b[0;31m#Get and clean the text from the job posting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/site-packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0mis_new_proxy_conn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_new_proxy_conn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/site-packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[1;32m    804\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tunnel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxy_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/site-packages/urllib3/connection.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# Calls self._set_hostport(), so self.host is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;31m# self._tunnel_host below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;31m# Mark this connection as not reusable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_tunnel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m         response = self.response_class(self.sock, strict = self.strict,\n\u001b[1;32m    795\u001b[0m                                        method = self._method)\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"HTTP/0.9\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# End the loop when all job state urls have been processed\n",
    "while len(final_state_url_dict.keys()) > 0:\n",
    "    for state in final_state_url_dict.keys():\n",
    "        try:\n",
    "            print(\"Number of states left to scrape: \"+str(len(final_state_url_dict.keys())))\n",
    "            time.sleep(1) # sleep 1 second\n",
    "\n",
    "            # Assign random proxy\n",
    "            proxy['https'] = randProxy()\n",
    "\n",
    "            # Assign random header\n",
    "            header['User-Agent'] = randHeader()\n",
    "            \n",
    "            #Access the job listing page for a state\n",
    "            response = requests.get(final_state_url_dict[state],proxies=proxy,headers=header,timeout=3) #Get the html code\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\") #Store html in a soup object \n",
    "\n",
    "            links = []\n",
    "            job_titles = {} \n",
    "            \n",
    "            # If the last two letters are not '=0' then there are actually job posts to scrape\n",
    "            if final_state_url_dict[state][-2:] != '=0':\n",
    "                # Access every URL within the page and store only urls for job openings\n",
    "                for link in soup.find_all('a'):\n",
    "                    if link.get('href')[0:21] == 'https://job-openings.': ## a job opening always begins with 'https://job-openings.monster.com'\n",
    "                        links.append(link.get('href'))\n",
    "                        job_titles[link.get('href')] = link.contents[0].strip('\\n').strip('\\r')\n",
    "            else: \n",
    "                #delete states job search url and end the loop early\n",
    "                del final_state_url_dict[state]\n",
    "                print state+'has no jobs'\n",
    "                break\n",
    "           \n",
    "            \n",
    "                \n",
    "            # Access each job posting and store it's html code into a beautiful soup object\n",
    "            # End the loop when all job title urls have been processed\n",
    "            while len(job_titles.keys()) > 0: \n",
    "                for url in job_titles.keys():\n",
    "                    \n",
    "                    try:\n",
    "                        # Assign random proxy\n",
    "                        proxy['https'] = randProxy()\n",
    "\n",
    "                        # Assign random header\n",
    "                        header['User-Agent'] = randHeader()\n",
    "\n",
    "                      #  count2 += 1\n",
    "                        print(\"Number of jobs left to scrape: \"+str(len(job_titles.keys())))\n",
    "\n",
    "                        response = requests.get(url,proxies=proxy,headers=header,timeout=3) #Get the html code\n",
    "                        soup = BeautifulSoup(response.text, \"html.parser\") #Store html in a soup object \n",
    "                        #Get and clean the text from the job posting\n",
    "                        text = soup.body.find_all('script')[1]\n",
    "                        # Convert to string -> Convert to lower case \n",
    "                        text_wrangled = str(text).lower()\n",
    "                        # replace (all non ascii characters,'.' ,'+' , and '3') with ','\n",
    "                        text_wrangled = re.sub(\"[^a-zA-Z.+#3]\",',',text_wrangled)\n",
    "                        # split string into a list separated by ','\n",
    "                        text_wrangled = text_wrangled.split(',')\n",
    "                        # remove all consecutive duplicates \n",
    "                        text_wrangled = groupby(text_wrangled)\n",
    "                        text_wrangled = [x[0] for x in text_wrangled]\n",
    "                        # Remove the stopwords and empty spaces\n",
    "                        text_final = [x for x in text_wrangled if not x in stop_words and not x =='']\n",
    "                        #Add the values to the Dictionary\n",
    "                        addValsToDict(state,job_titles[url])\n",
    "\n",
    "                        #delete the job post link when finished\n",
    "                        del job_titles[url]\n",
    "                        \n",
    "                    except Exception,e: \n",
    "                        print str(e)\n",
    "                        \n",
    "            #delete states job search url\n",
    "            del final_state_url_dict[state]\n",
    "       \n",
    "        except Exception,e: \n",
    "            print str(e)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "date = str(now.month)+'-'+str(now.day)+'-'+str(now.year)\n",
    "data.to_csv('scraped_data_'+search_term+'_'+date+'.txt',encoding='utf-8',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scraped_data_'+search_term+'_'+date+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
